{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77eafc98",
   "metadata": {},
   "source": [
    "# **Assignment 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ccc253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "DATA_DIR = Path(\"Census Income Data Set\")\n",
    "cols = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\n",
    "        \"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\n",
    "        \"hours-per-week\",\"native-country\",\"income\"]\n",
    "\n",
    "def load_adult(train_path, test_path):\n",
    "    tr = pd.read_csv(train_path, names=cols, na_values=\"?\", skipinitialspace=True)\n",
    "    te = pd.read_csv(test_path,  names=cols, na_values=\"?\", skipinitialspace=True)\n",
    "    for df in (tr, te):\n",
    "        df[\"income\"] = df[\"income\"].astype(str).str.replace(\".\",\"\",regex=False).str.strip()\n",
    "    tr = tr.dropna(subset=[\"income\"]).reset_index(drop=True)\n",
    "    te = te.dropna(subset=[\"income\"]).reset_index(drop=True)\n",
    "    return tr, te\n",
    "\n",
    "train_df, test_df = load_adult(DATA_DIR / \"adult.data\", DATA_DIR / \"adult.test\")\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = (train_df[\"income\"] == \">50K\").astype(int)\n",
    "y_test  = (test_df[\"income\"]  == \">50K\").astype(int)\n",
    "X_train = train_df.drop(columns=[\"income\"])\n",
    "X_test  = test_df.drop(columns=[\"income\"])\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Numeric features:\", num_cols)\n",
    "print(\"Categorical features:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bdd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 5: Preprocessing Pipelines =====\n",
    "# This version works with sklearn >= 1.2 (uses sparse_output instead of sparse)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Logistic Regression: numeric features need scaling\n",
    "numeric_transformer_lr = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# XGBoost: numeric features do not need scaling\n",
    "numeric_transformer_xgb = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "# Categorical transformer (common for both)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))  # <-- use sparse_output here\n",
    "])\n",
    "\n",
    "# Combine into ColumnTransformers\n",
    "preprocess_lr = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer_lr, num_cols),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "preprocess_xgb = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer_xgb, num_cols),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "print(\"✅ Preprocessing pipelines created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd625a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 6: Model Setup and Tuning =====\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 3-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=6227)\n",
    "\n",
    "# Logistic Regression pipeline\n",
    "lr_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess_lr),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"saga\", n_jobs=-1))\n",
    "])\n",
    "\n",
    "lr_param_dist = {\n",
    "    \"clf__C\": [0.1, 0.5, 1, 2, 3],\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "lr_search = RandomizedSearchCV(\n",
    "    estimator=lr_pipe,\n",
    "    param_distributions=lr_param_dist,\n",
    "    n_iter=6,\n",
    "    scoring=\"f1\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=6227,\n",
    "    refit=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# XGBoost pipeline\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess_xgb),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.15,\n",
    "        max_depth=6,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=6227\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_param_dist = {\n",
    "    \"clf__n_estimators\": [300, 400, 600],\n",
    "    \"clf__learning_rate\": [0.1, 0.15, 0.2],\n",
    "    \"clf__max_depth\": [4, 6, 8],\n",
    "    \"clf__subsample\": [0.8, 0.9, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "    \"clf__reg_lambda\": [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipe,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=8,\n",
    "    scoring=\"f1\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=6227,\n",
    "    refit=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"✅ Models lr_search and xgb_search are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c32c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 7: Train and Evaluate both models =====\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, average_precision_score,\n",
    "                             roc_curve, precision_recall_curve)\n",
    "\n",
    "def train_and_eval(estimator, name, Xtr, ytr, Xte, yte):\n",
    "    \"\"\"Train a model, record timing, compute metrics and curves.\"\"\"\n",
    "    # --- Training ---\n",
    "    t0 = time.perf_counter()\n",
    "    estimator.fit(Xtr, ytr)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # --- Prediction ---\n",
    "    y_pred = estimator.predict(Xte)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    # --- Scores for curves/AUC ---\n",
    "    try:\n",
    "        y_score = estimator.predict_proba(Xte)[:, 1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            y_score = estimator.decision_function(Xte)\n",
    "        except Exception:\n",
    "            y_score = y_pred.astype(float)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    acc = accuracy_score(yte, y_pred)\n",
    "    pre = precision_score(yte, y_pred, zero_division=0)\n",
    "    rec = recall_score(yte, y_pred, zero_division=0)\n",
    "    f1  = f1_score(yte, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(yte, y_score)\n",
    "    ap  = average_precision_score(yte, y_score)\n",
    "\n",
    "    # --- Curves ---\n",
    "    fpr, tpr, _ = roc_curve(yte, y_score)\n",
    "    pr_p, pr_r, _ = precision_recall_curve(yte, y_score)\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc, \"Precision\": pre, \"Recall\": rec, \"F1\": f1,\n",
    "        \"ROC_AUC\": auc, \"PR_AUC(AP)\": ap,\n",
    "        \"Train(s)\": t1 - t0, \"Predict(s)\": t2 - t1,\n",
    "        \"FPR\": fpr, \"TPR\": tpr, \"PR_Precision\": pr_p, \"PR_Recall\": pr_r\n",
    "    }\n",
    "\n",
    "# --- Clean numeric columns in X_train and X_test ---\n",
    "def clean_numeric(df, num_cols):\n",
    "    df_clean = df.copy()\n",
    "    for col in num_cols:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    return df_clean\n",
    "\n",
    "X_train_clean = clean_numeric(X_train, num_cols)\n",
    "X_test_clean  = clean_numeric(X_test, num_cols)\n",
    "\n",
    "# Remove rows with NaN in any numeric column\n",
    "train_mask = X_train_clean[num_cols].notnull().all(axis=1)\n",
    "test_mask  = X_test_clean[num_cols].notnull().all(axis=1)\n",
    "\n",
    "X_train_final = X_train_clean[train_mask].reset_index(drop=True)\n",
    "y_train_final = y_train[train_mask].reset_index(drop=True)\n",
    "X_test_final  = X_test_clean[test_mask].reset_index(drop=True)\n",
    "y_test_final  = y_test[test_mask].reset_index(drop=True)\n",
    "\n",
    "# --- Train both models and collect results ---\n",
    "summaries = []\n",
    "summaries.append(train_and_eval(lr_search,  \"Logistic Regression (tuned)\", X_train_final, y_train_final, X_test_final, y_test_final))\n",
    "summaries.append(train_and_eval(xgb_search, \"XGBoost (tuned)\",              X_train_final, y_train_final, X_test_final, y_test_final))\n",
    "\n",
    "# Display summary table\n",
    "import pandas as pd\n",
    "pd.DataFrame([{k:(v if isinstance(v,(str,int,float)) else '...') for k,v in s.items()} for s in summaries])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure()\n",
    "for s in summaries:\n",
    "    plt.plot(s[\"FPR\"], s[\"TPR\"], label=f'{s[\"Model\"]} (AUC={s[\"ROC_AUC\"]:.3f})')\n",
    "plt.plot([0,1],[0,1],'--',linewidth=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Test)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision–Recall Curve\n",
    "plt.figure()\n",
    "for s in summaries:\n",
    "    plt.plot(s[\"PR_Recall\"], s[\"PR_Precision\"], label=f'{s[\"Model\"]} (AP={s[\"PR_AUC(AP)\"]:.3f})')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve (Test)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(estimator, X_test, y_test, model_name):\n",
    "    \"\"\"Plot confusion matrix for a fitted model.\"\"\"\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"<=50K\", \">50K\"])\n",
    "    disp.plot(cmap=\"Blues\", values_format='d')\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()\n",
    "    return cm\n",
    "\n",
    "# Logistic Regression\n",
    "cm_lr = plot_confusion_matrix(lr_search, X_test_final, y_test_final, \"Logistic Regression (tuned)\")\n",
    "\n",
    "# XGBoost\n",
    "cm_xgb = plot_confusion_matrix(xgb_search, X_test_final, y_test_final, \"XGBoost (tuned)\")\n",
    "\n",
    "# Print numeric results\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", cm_lr)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", cm_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b28f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
